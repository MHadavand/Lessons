{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winery classification using the one-dimensional Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian distribution is used to model the conditional probability of each feature given a specific label value. Bayesian inversion can be used to get the conditional probability of a label given the value of features. In this notebook, each variable is examined independently with fitting a univariate Gaussian distribution.\n",
    "\n",
    "The data can be downloaded from the UCI repository (https://archive.ics.uci.edu/ml/datasets/wine). It contains 178 labeled data points, each corresponding to a bottle of wine:\n",
    "* The features (`x`): a 13-dimensional vector consisting of visual and chemical features for the bottle of wine\n",
    "* The label (`y`): the winery from which the bottle came (1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the packages we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:34.274776Z",
     "start_time": "2020-05-26T19:18:32.846652Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard includes\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Useful module for dealing with the Gaussian density\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "# installing packages for interactive graphs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the Wine data set. There are 178 data points, each with 13 features and a label (1,2,3).\n",
    "We will divide these into a training set of 130 points and a test set of 48 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:34.292671Z",
     "start_time": "2020-05-26T19:18:34.282648Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'wine.data.txt' needs to be in the same directory\n",
    "data = np.loadtxt('wine.data.txt', delimiter=',')\n",
    "# Names of features\n",
    "featurenames = ['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash','Magnesium', 'Total phenols', \n",
    "                'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', \n",
    "                'OD280/OD315 of diluted wines', 'Proline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix a particular \"random\" permutation of the data, and use these to effect the training / test split.\n",
    "We get four arrays:\n",
    "* `trainx`: 130x13, the training points\n",
    "* `trainy`: 130x1, labels of the training points\n",
    "* `testx`: 48x13, the test points\n",
    "* `testy`: 48x1, labels of the test points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:34.305666Z",
     "start_time": "2020-05-26T19:18:34.296646Z"
    }
   },
   "outputs": [],
   "source": [
    "n_total = len(data)\n",
    "n_features = len(featurenames)\n",
    "classes = np.unique(data[:,0])\n",
    "n_classes = len(np.unique(data[:,0]))\n",
    "n_train = 130\n",
    "n_test = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:34.317650Z",
     "start_time": "2020-05-26T19:18:34.308649Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split 178 instances into training set (trainx, trainy) of size 130 and test set (testx, testy) of size 48\n",
    "# Also split apart data and labels\n",
    "np.random.seed(0)\n",
    "perm = np.random.permutation(n_total)\n",
    "trainx = data[perm[0:n_train],1:n_features+1]\n",
    "trainy = data[perm[0:n_train],0]\n",
    "testx = data[perm[n_train:], 1:n_features+1]\n",
    "testy = data[perm[n_train:],0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check probability of each class for training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:34.334675Z",
     "start_time": "2020-05-26T19:18:34.321664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of label 1: 43 %33.08\n",
      "Total number of label 2: 54 %41.54\n",
      "Total number of label 3: 33 %25.38\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_classes):\n",
    "    print('Total number of label {}: {} %{:.2f}'.format(i+1, sum(trainy==i+1), 100*sum(trainy==i+1)/n_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check probability of each class for test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:35.145451Z",
     "start_time": "2020-05-26T19:18:35.137448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of label 1: 16 %33.33\n",
      "Total number of label 2: 17 %35.42\n",
      "Total number of label 3: 15 %31.25\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_classes):\n",
    "    print('Total number of label {}: {} %{:.2f}'.format(i+1, sum(testy==i+1), 100*sum(testy==i+1)/n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the distribution of a each feature from one of the wineries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:36.068825Z",
     "start_time": "2020-05-26T19:18:35.834822Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dfe3b69d85460a93162d490fc9d9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='feature', max=13, min=1), IntSlider(value=1, description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact( feature=IntSlider(1,1,13), label=IntSlider(1,1,3))\n",
    "def density_plot(feature, label):\n",
    "    feature = feature -1\n",
    "    fig, ax = plt.subplots(1,1, figsize = (8,5))\n",
    "    ax.hist(trainx[trainy==label,feature], density=True, facecolor='orange')\n",
    "    #\n",
    "    mu = np.mean(trainx[trainy==label,feature]) # mean\n",
    "    var = np.var(trainx[trainy==label,feature]) # variance\n",
    "    std = np.sqrt(var) # standard deviation\n",
    "    \n",
    "    # 99.7% of the data \n",
    "    x_axis = np.linspace(mu - 3*std, mu + 3*std, 1000)\n",
    "    ax.plot(x_axis, norm.pdf(x_axis,mu,std), 'b', lw=2)\n",
    "    ax.set_title(\"Winery \"+str(label) )\n",
    "    ax.text(x=0.05, y=0.8, s='mean: {:.2f}, std: {:.2f}'.format(mu, std), transform = ax.transAxes)\n",
    "    ax.set_xlabel(featurenames[feature], fontsize=14, color='k')\n",
    "    ax.set_ylabel('Density', fontsize=14, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the feature with least variability for each wine type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:36.863107Z",
     "start_time": "2020-05-26T19:18:36.826324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8650775da5bc4c09b8131a92141d662d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='label', max=3, min=1), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the minimum variability for each class\n",
    "@interact(label=IntSlider(1,1,3))\n",
    "def get_min_variability(label):\n",
    "    std = np.zeros(n_features)\n",
    "    for feature in range(n_features):\n",
    "        std[feature] = np.std(trainx[trainy==label,feature])\n",
    "\n",
    "    print('Feature {:g} has the minimum standard deviation ({:.2f})'.format(np.argmin(std), np.min(std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit a Gaussian to each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that will fit a Gaussian generative model to the three classes, given a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:38.124090Z",
     "start_time": "2020-05-26T19:18:38.115063Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_generative_model(x,y,feature):\n",
    "    feature =feature - 1 # Index\n",
    "    mu = {}\n",
    "    var ={}\n",
    "    pi = {}\n",
    "    for label in range(n_classes):\n",
    "        label+=1\n",
    "        indices = (y==label)\n",
    "        mu.update({label:np.mean(x[indices,feature])})\n",
    "        \n",
    "        var.update({label:np.var(x[indices,feature])})\n",
    "        \n",
    "        pi.update({label:float(sum(indices))/float(len(y))})\n",
    "    return mu, var, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, display the Gaussian distribution for each of the three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:39.339587Z",
     "start_time": "2020-05-26T19:18:39.141603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a8cde4bafc47f4b2ae9d6054ee2592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='feature', max=13, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact( feature=IntSlider(1,1,13) )\n",
    "def show_densities(feature):\n",
    "    fig, ax = plt.subplots(1,1, figsize = (8,5))\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "    \n",
    "    colors = ['r', 'k', 'g']\n",
    "    \n",
    "    for label in range(n_classes):\n",
    "        m = mu[label+1]\n",
    "        s = np.sqrt(var[label+1])\n",
    "        x_axis = np.linspace(m - 3*s, m+3*s, 1000)\n",
    "        ax.plot(x_axis, norm.pdf(x_axis,m,s), colors[label], label=\"class \" + str(label))\n",
    "    ax.set_xlabel(featurenames[feature-1], fontsize=14, color='k')\n",
    "    ax.set_ylabel('Density', fontsize=14, color='k')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted Gaussian distributions/kernels can be used to evaluate the overlap between different classes for each feature. The feature with least overlap should provide a better accuracy as it results in distinction between different wine types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict labels for the test set\n",
    "\n",
    "Using Bayesian inversion we have\"\n",
    "\n",
    "$ P(Y=y_j | X ) = \\frac{P(X| Y=y_j). P(Y=y_j) }{P(X)} $\n",
    "\n",
    "Since the denominator is the same for all the labels/wine types, we can ignore that and find the use the majority rule decision and classify the wine given the feature value (i.e. $x$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T19:18:41.678922Z",
     "start_time": "2020-05-26T19:18:41.604926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e1dd15255348c187e0ac78965c1fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='feature', max=13, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact( feature=IntSlider(1,1,13)  )\n",
    "def test_model(feature):\n",
    "    mu, var, pi = fit_generative_model(trainx, trainy, feature)\n",
    "\n",
    "    score = np.zeros((n_test,n_classes))\n",
    "    for i in range(n_test):\n",
    "        for label in range(n_classes):\n",
    "            # Using log probaility and summation instead of multiplocation\n",
    "            score[i,label] = np.log(pi[label+1]) + \\\n",
    "            norm.logpdf(testx[i,feature-1], mu[label+1], np.sqrt(var[label+1])) \n",
    "    predictions = np.argmax(score, axis=1) + 1\n",
    "    # Finally, tally up score\n",
    "    errors = np.sum(predictions != testy)\n",
    "    print (\"Test error using feature \" + featurenames[feature-1] + \": \" + str(errors) + \"/\" + str(n_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
